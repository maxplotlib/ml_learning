{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5715ea7",
   "metadata": {},
   "source": [
    "# Putting the LLM Pipeline Together : Step by Step\n",
    "\n",
    "Complete process of text generation with a local LLM :\n",
    "\n",
    "Input text → Tokenization → Converting to IDs → Model processing → Next token prediction → Token selection → Building the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86300b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "llm_directory = \"./downloaded_llms/distilgpt2_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_directory)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12beb2ea",
   "metadata": {},
   "source": [
    "## Step 1: Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505ab630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple prompt\n",
    "prompt = \"Artificial intelligence is transforming\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b3a54",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenization - Breaking Text into Pieces\n",
    "The tokenizer breaks our text into smaller units (tokens) that the model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72da02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Result : ['Art', 'ificial', 'Ġintelligence', 'Ġis', 'Ġtransforming']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(prompt)\n",
    "print(f\"Tokenization Result : {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8562fd",
   "metadata": {},
   "source": [
    "The tokenizer has split the input text into tokens :\n",
    "- Some tokens have a 'Ġ' prefix - this represents a space before the word\n",
    "- The word \"transforming\" is kept as a single token because it's common enough\n",
    "- If we used a less common word, it might be split into multiple subword tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d2e31",
   "metadata": {},
   "source": [
    "## Step 3 : Converting Tokens to IDs\n",
    "Next, each token is converted to its corresponding numeric ID from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2d3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID :\n",
      "- 'Art' -> ID 8001\n",
      "- 'ificial' -> ID 9542\n",
      "- 'Ġintelligence' -> ID 4430\n",
      "- 'Ġis' -> ID 318\n",
      "- 'Ġtransforming' -> ID 25449\n",
      "\n",
      "Model input tensor shape: torch.Size([1, 5])\n",
      "Model input tensor: tensor([[ 8001,  9542,  4430,   318, 25449]])\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to IDs\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"Token to ID :\")\n",
    "for token,ids in zip(tokens, input_ids[0].tolist()):\n",
    "    print(f\"- '{token}' -> ID {ids}\")\n",
    "    \n",
    "# Show the tensor format that will be input to the model\n",
    "print()\n",
    "print(\"Model input tensor shape:\", input_ids.shape)\n",
    "print(\"Model input tensor:\", input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8dd19f",
   "metadata": {},
   "source": [
    "Each token has been converted to a numeric ID according to the model's vocabulary. \n",
    "\n",
    "These IDs are what the model actually processes - it doesn't understand the text directly, only these numbers.\n",
    "\n",
    "The IDs are then formatted as a PyTorch tensor with shape [1, n_tokens] - this is the actual input format the model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327c748",
   "metadata": {},
   "source": [
    "## Step 4: Model Processing\n",
    "Now the model processes these IDs through its neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec69a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Output logits shape : torch.Size([1, 5, 50257])\n",
      "- This means we have predictions for 5 positions\n",
      "- For each position, we have scores for all 50257 tokens in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Run the model on our input\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "# The model outputs logits (unnormalized probabilities) for each possible next token\n",
    "logits = output.logits\n",
    "print(f\"- Output logits shape : {logits.shape}\")\n",
    "print(f\"- This means we have predictions for {logits.shape[1]} positions\")\n",
    "print(f\"- For each position, we have scores for all {logits.shape[2]} tokens in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd8dca",
   "metadata": {},
   "source": [
    "Inside the model, here's what's happening\n",
    "\n",
    "- The number IDs for each word get turned into lists of numbers that represent their meaning (each ID is converted into an embedding vector).\n",
    "- The model adds information about where each word appears in the sentence - first, second, third, etc. (with position embeddings).\n",
    "- The model then processes this information through several layers that:\n",
    "    - Figure out which words should pay attention to each other - like how \"is\" relates to \"intelligence\" (self-attention mechanisms).\n",
    "    - Process this information to understand the meaning better\n",
    "- Finally, the model makes a giant list of scores for every possible next word it knows.\n",
    "\n",
    "The output is basically a big scorecard showing how likely each possible next word is.\n",
    "\n",
    "Since this model knows about 50,257 different words or word pieces, it gives a score to each one of them, ranking from most likely to least likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118f6d2",
   "metadata": {},
   "source": [
    "## Step 5 : Next Token Prediction\n",
    "Now let's look at the model's prediction for the next token after the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5dff361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Predictions for Next Token :\n",
      "----------------------------------------\n",
      "Token           ID       Probability\n",
      "----------------------------------------\n",
      "' the'          262      26.98%\n",
      "' our'          674      6.10%\n",
      "' human'        1692     2.45%\n",
      "' people'       661      2.02%\n",
      "' itself'       2346     1.83%\n",
      "' technology'   3037     1.82%\n",
      "' a'            257      1.54%\n",
      "' us'           514      1.41%\n",
      "' society'      3592     1.29%\n",
      "' how'          703      1.24%\n"
     ]
    }
   ],
   "source": [
    "# We want the predictions for the last position (after \"transforming\")\n",
    "next_token_logit = logits[0, -1, :]\n",
    "\n",
    "# Convert logits to probabilities\n",
    "next_token_proba = torch.softmax(next_token_logit, dim=0)\n",
    "\n",
    "# Get the top 10 most likely tokens\n",
    "top_k = 10\n",
    "top_k_proba, top_k_idx = torch.topk(next_token_proba, top_k)\n",
    "\n",
    "# Convert to lists for easier handling\n",
    "top_k_proba = top_k_proba.detach().numpy()\n",
    "top_k_idx = top_k_idx.detach().numpy()\n",
    "\n",
    "# Get the corresponding tokens\n",
    "top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_idx]\n",
    "print(\"Top 10 Predictions for Next Token :\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Token':<15} {'ID':<8} {'Probability':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(top_k):\n",
    "    print(f\"{repr(top_k_tokens[i]):<15} {top_k_idx[i]:<8} {top_k_proba[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38aff7",
   "metadata": {},
   "source": [
    "The model has read the prompt phrase \"Artificial intelligence is transforming\" and made a guess about what might come next:\n",
    "- First, the model creates raw scores for every possible next tokens\n",
    "- Softmax turns these scores into probabilities (like 60%, 25%, 10%) so they're easier to understand\n",
    "- Look at just the top 10 tokens with the highest proba\n",
    "\n",
    "These probabilities show what the model thinks should come next based on all the text it's seen before. \n",
    "\n",
    "A higher percentage means the model is more confident that word is a good fit to continue the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b674590",
   "metadata": {},
   "source": [
    "## Step 6 : Token Selection\n",
    "Now we need to select which token to use next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c62bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy selection : ' the' (always picks the most likely token)\n",
      "Temperature sampling : ' the' (randomly selects based on adjusted probabilities)\n",
      "\n",
      "Top-k tokens with temperature adjustment :\n",
      "----------------------------------------\n",
      "Token           Original %   Adjusted %  \n",
      "----------------------------------------\n",
      "' the'          26.98        83.47       \n",
      "' our'          6.10         9.97        \n",
      "' human'        2.45         2.71        \n",
      "' people'       2.02         2.06        \n",
      "' itself'       1.83         1.79        \n"
     ]
    }
   ],
   "source": [
    "# Method 1: Greedy selection (always pick the most likely token)\n",
    "greedy_idx = torch.argmax(next_token_proba)\n",
    "greedy_token = tokenizer.decode(greedy_idx)\n",
    "\n",
    "# Method 2: Temperature sampling (adjust probability distribution)\n",
    "temperature = .7 # Lower = more deterministic, Higher = more random\n",
    "temp_logits = next_token_logit / temperature\n",
    "temp_token_proba = torch.softmax(temp_logits, dim=0)\n",
    "\n",
    "# Method 3: Top-k sampling (sample from k most likely tokens)\n",
    "k = 5\n",
    "temp_proba, temp_idx = torch.topk(temp_token_proba, k)\n",
    "top_k_temp_proba = temp_proba / temp_proba.sum()\n",
    "\n",
    "# Let's select using temperature + top-k\n",
    "sample_idx = np.random.choice(temp_idx.numpy(), p=top_k_temp_proba.numpy())\n",
    "sample_token = tokenizer.decode(sample_idx)\n",
    "\n",
    "print(f\"Greedy selection : '{greedy_token}' (always picks the most likely token)\")\n",
    "print(f\"Temperature sampling : '{sample_token}' (randomly selects based on adjusted probabilities)\")\n",
    "print()\n",
    "# Show the top-k tokens with adjusted probabilities\n",
    "print(\"Top-k tokens with temperature adjustment :\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Token':<15} {'Original %':<12} {'Adjusted %':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(k):\n",
    "    token_id = temp_idx[i].item()\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    orig_prob = next_token_proba[token_id].item() * 100\n",
    "    adj_prob = top_k_temp_proba[i].item() * 100\n",
    "    print(f\"{repr(token_text):<15} {orig_prob:<12.2f} {adj_prob:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c8d18",
   "metadata": {},
   "source": [
    "two different ways to pick the next word :\n",
    "1. **Greedy selection** : Always pick the most likely word. This is like always picking the safe choice. It's predictable, but can get boring and repetitive\n",
    "2. **Temperature sampling** with Top-k : Mix in some controlled randomness\n",
    "\n",
    "- We can adjust how random we want to be (temperature)\n",
    "- We only consider the few most likely words (top-k)\n",
    "- Then randomly choose from those words, giving better odds to more likely words\n",
    "\n",
    "Using some randomness helps make the text more interesting and varied, instead of always saying the same thing when given the same starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123476e",
   "metadata": {},
   "source": [
    "## Step 7 : Building the Response\n",
    "Complete text generation process in action, adding one token at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d57471a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_by_step(prompt, max_new_tokens=5, temperature=.7, top_k=5):\n",
    "    \"\"\"\n",
    "        Generate text token by token with detailed output at each step\n",
    "\n",
    "    Args:\n",
    "        prompt (str): input\n",
    "        max_new_tokens (int, optional): number of token to predict. Defaults to 5.\n",
    "        temprerature (float, optional): level of randomness. Defaults to .7.\n",
    "        top_k (int, optional): tokens with highest probabilities. Defaults to 10.\n",
    "    \"\"\"\n",
    "    # Start with the prompt\n",
    "    current_text = prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(f\"Starting prompt : '{prompt}'\")\n",
    "    print()\n",
    "    \n",
    "    # Generate new tokens one by one\n",
    "    for i in range(max_new_tokens):\n",
    "        print(f\"--- Step {i + 1} : Generating token #{len(prompt.split()) + i + 1} ---\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get next token logits (predictions for the next token)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get top-k token indices and their probabilities\n",
    "        tokens_proba = torch.softmax(next_token_logits, dim=0)\n",
    "        top_k_proba, top_k_idx = torch.topk(tokens_proba, top_k)\n",
    "        \n",
    "        # Print the top candidates\n",
    "        print()\n",
    "        print(\"Top candidates :\")\n",
    "        for i in range(top_k):\n",
    "            token_id = top_k_idx[i].item()\n",
    "            token_text = tokenizer.decode(token_id)\n",
    "            token_proba = top_k_proba[i]\n",
    "            print(f\"- Token {i + 1} : {token_text}, (ID : {token_id}), Probability : {token_proba * 100:.2f}\")\n",
    "            \n",
    "        # Renormalize probabilities for top-k\n",
    "        top_k_proba = top_k_proba / top_k_proba.sum()\n",
    "        \n",
    "        # Sample from top-k\n",
    "        sample_idx = np.random.choice(top_k_idx.numpy(), p=top_k_proba.numpy())\n",
    "        sample_token = tokenizer.decode([sample_idx])\n",
    "        \n",
    "        print()\n",
    "        print(f\"Selected token : '{sample_token}'\")\n",
    "        \n",
    "        # Update for next iteration\n",
    "        next_token = torch.tensor([[sample_idx]])\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        current_text = current_text + sample_token\n",
    "        \n",
    "        print(f\"Text so far : '{current_text}'\")\n",
    "        print()\n",
    "    print(f\"Final generated text : '{current_text}'\")\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "223eaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt : 'Artificial intelligence is transforming'\n",
      "\n",
      "--- Step 1 : Generating token #5 ---\n",
      "\n",
      "Top candidates :\n",
      "- Token 1 :  the, (ID : 262), Probability : 66.72\n",
      "- Token 2 :  our, (ID : 674), Probability : 7.97\n",
      "- Token 3 :  human, (ID : 1692), Probability : 2.16\n",
      "- Token 4 :  people, (ID : 661), Probability : 1.65\n",
      "- Token 5 :  itself, (ID : 2346), Probability : 1.43\n",
      "\n",
      "Selected token : ' the'\n",
      "Text so far : 'Artificial intelligence is transforming the'\n",
      "\n",
      "--- Step 2 : Generating token #6 ---\n",
      "\n",
      "Top candidates :\n",
      "- Token 1 :  way, (ID : 835), Probability : 45.63\n",
      "- Token 2 :  world, (ID : 995), Probability : 33.67\n",
      "- Token 3 :  lives, (ID : 3160), Probability : 4.88\n",
      "- Token 4 :  human, (ID : 1692), Probability : 2.45\n",
      "- Token 5 :  workplace, (ID : 15383), Probability : 0.91\n",
      "\n",
      "Selected token : ' world'\n",
      "Text so far : 'Artificial intelligence is transforming the world'\n",
      "\n",
      "--- Step 3 : Generating token #7 ---\n",
      "\n",
      "Top candidates :\n",
      "- Token 1 :  of, (ID : 286), Probability : 37.49\n",
      "- Token 2 : ., (ID : 13), Probability : 18.31\n",
      "- Token 3 :  into, (ID : 656), Probability : 12.98\n",
      "- Token 4 : ,, (ID : 11), Probability : 7.24\n",
      "- Token 5 :  around, (ID : 1088), Probability : 6.34\n",
      "\n",
      "Selected token : ' of'\n",
      "Text so far : 'Artificial intelligence is transforming the world of'\n",
      "\n",
      "--- Step 4 : Generating token #8 ---\n",
      "\n",
      "Top candidates :\n",
      "- Token 1 :  the, (ID : 262), Probability : 15.25\n",
      "- Token 2 :  science, (ID : 3783), Probability : 11.44\n",
      "- Token 3 :  technology, (ID : 3037), Probability : 8.57\n",
      "- Token 4 :  artificial, (ID : 11666), Probability : 7.56\n",
      "- Token 5 :  computing, (ID : 14492), Probability : 5.33\n",
      "\n",
      "Selected token : ' science'\n",
      "Text so far : 'Artificial intelligence is transforming the world of science'\n",
      "\n",
      "--- Step 5 : Generating token #9 ---\n",
      "\n",
      "Top candidates :\n",
      "- Token 1 :  and, (ID : 290), Probability : 45.56\n",
      "- Token 2 : ., (ID : 13), Probability : 19.95\n",
      "- Token 3 :  into, (ID : 656), Probability : 11.01\n",
      "- Token 4 : ,, (ID : 11), Probability : 9.08\n",
      "- Token 5 :  fiction, (ID : 10165), Probability : 6.26\n",
      "\n",
      "Selected token : ' and'\n",
      "Text so far : 'Artificial intelligence is transforming the world of science and'\n",
      "\n",
      "Final generated text : 'Artificial intelligence is transforming the world of science and'\n"
     ]
    }
   ],
   "source": [
    "# Generate text step by step\n",
    "final_text = generate_step_by_step(prompt, max_new_tokens=5, temperature=.7, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9d7f8",
   "metadata": {},
   "source": [
    "Complete text generation process, with each step broken down :\n",
    "\n",
    "- Start with a prompt\n",
    "- For each new token :\n",
    "    - The model processes all the text so far\n",
    "    - It generates probabilities for the next token\n",
    "    - We apply temperature and top-k filtering\n",
    "    - We sample a token from the resulting distribution\n",
    "    - The selected token is added to our text\n",
    "    - Repeat until we reach our desired length\n",
    "    \n",
    "This shows how the model works in an auto-regressive manner - each new token depends on all the tokens that came before it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270b1eb",
   "metadata": {},
   "source": [
    "## The Effect of Generation Parameters\n",
    "Different parameters can dramatically change the output, let's experiment with a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c50e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of Generation Parameters :\n",
      "\n",
      "Greedy (no sampling)\n",
      "Parameters: {'do_sample': False}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the way we think about our lives.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Low Temperature (0.3)\n",
      "Parameters: {'temperature': 0.3, 'do_sample': True}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the way we think about the world.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "High Temperature (1.5)\n",
      "Parameters: {'temperature': 1.5, 'do_sample': True}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the way humans navigate the world, and we may still learn a thing or\n",
      "--------------------------------------------------------------------------------\n",
      "Top-k (5)\n",
      "Parameters: {'top_k': 5, 'do_sample': True}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the way we live, we know it, we are living, we know\n",
      "--------------------------------------------------------------------------------\n",
      "Top-p (0.9)\n",
      "Parameters: {'top_p': 0.9, 'do_sample': True}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the world, in ways that don't make it appear to be making any\n",
      "--------------------------------------------------------------------------------\n",
      "Balanced\n",
      "Parameters: {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}\n",
      "Input: Artificial intelligence is transforming\n",
      "Generated:  the world into a real world, and it’s becoming a world\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to generate text with different parameters\n",
    "def generate_with_params(prompt, max_new_tokens=15, **params):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Set up generation parameters\n",
    "    gen_params = {}\n",
    "    if 'temperature' in params:\n",
    "        gen_params['temperature'] = params['temperature']\n",
    "    if 'top_k' in params:\n",
    "        gen_params['top_k'] = params['top_k']\n",
    "    if 'top_p' in params:\n",
    "        gen_params['top_p'] = params['top_p']\n",
    "    if 'do_sample' in params:\n",
    "        gen_params['do_sample'] = params['do_sample']\n",
    "    \n",
    "    # Generate the output\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=len(input_ids[0]) + max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **gen_params\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Try different parameter combinations\n",
    "params_to_try = [\n",
    "    {'name': 'Greedy (no sampling)', 'params': {'do_sample': False}},\n",
    "    {'name': 'Low Temperature (0.3)', 'params': {'temperature': 0.3, 'do_sample': True}},\n",
    "    {'name': 'High Temperature (1.5)', 'params': {'temperature': 1.5, 'do_sample': True}},\n",
    "    {'name': 'Top-k (5)', 'params': {'top_k': 5, 'do_sample': True}},\n",
    "    {'name': 'Top-p (0.9)', 'params': {'top_p': 0.9, 'do_sample': True}},\n",
    "    {'name': 'Balanced', 'params': {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}}\n",
    "]\n",
    "\n",
    "# Generate and display results\n",
    "print(\"Effect of Generation Parameters :\")\n",
    "print()\n",
    "\n",
    "for setting in params_to_try:\n",
    "    output = generate_with_params(prompt, **setting['params'])\n",
    "    generated_part = output[len(prompt):]\n",
    "    \n",
    "    print(f\"{setting['name']}\")\n",
    "    print(f\"Parameters: {setting['params']}\")\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_part}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e383f6b",
   "metadata": {},
   "source": [
    "- **do_sample** : When False, the model always picks the most likely token (greedy decoding). When True, it samples according to the probability distribution\n",
    "\n",
    "- **temperature** : Controls the randomness of predictions\n",
    "    - Lower values (e.g., 0.3) make the model more confident and deterministic\n",
    "    - Higher values (e.g., 1.5) make the model more random and creative\n",
    "    - Value of 1.0 keeps the original probabilities unchanged\n",
    "\n",
    "- **top_k** : Limits the selection to only the k most likely next tokens\n",
    "    - Lower values (e.g., 5) focus on the most probable tokens\n",
    "    - Higher values allow more diversity but might include less relevant tokens\n",
    "\n",
    "- **top_p** (nucleus sampling) : Selects from the smallest set of tokens whose cumulative probability exceeds p\n",
    "    - Adapts the number of tokens considered based on the confidence of the model\n",
    "    - Values around 0.9 are common and work well in practice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
