{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcb0c05",
   "metadata": {},
   "source": [
    "# Running Inference Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af02e0c",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) have revolutionized AI applications, but they don't always need to be accessed through cloud APIs. \n",
    "\n",
    "In this notebook we download, save and run LLMs locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f81f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe35b8",
   "metadata": {},
   "source": [
    "### Running LLMs locally offers several advantages:\n",
    "\n",
    "- **Privacy** : Your data doesn't leave your environment\n",
    "- **Cost** : No per-token API charges\n",
    "- **Latency** : No network delays\n",
    "- **Customization** : Full control over model parameters\n",
    "\n",
    "### However, local LLMs also have limitations:\n",
    "\n",
    "- **Hardware requirements** : Models need sufficient RAM and GPU\n",
    "- **Model size** : Smaller models fit locally but may have reduced capabilities\n",
    "- **Updates** : You manage model versions yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c6722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face Hub...\n",
      "\n",
      "Model name : distilgpt2\n",
      "Number of parameters : 81912576\n",
      "Model size on disk : 312.47 MB (estimated)\n",
      "\n",
      "Saving model to : ./downloaded_llms/distilgpt2_model\n",
      "model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set the directory to save the model\n",
    "save_directory = \"./downloaded_llms/distilgpt2_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Downloading model from Hugging Face Hub...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print model information\n",
    "print()\n",
    "print(f\"Model name : {model_name}\")\n",
    "print(f\"Number of parameters : {model.num_parameters()}\")\n",
    "print(f\"Model size on disk : {model.num_parameters() * 4 / (1024 * 1024):.2f} MB (estimated)\")\n",
    "print()\n",
    "\n",
    "# Save the model and tokenizer to the specified directory\n",
    "print(f\"Saving model to : {save_directory}\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b120e",
   "metadata": {},
   "source": [
    "## Loading and Using a Local Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70e56",
   "metadata": {},
   "source": [
    "Once the model is saved, it can be loaded from local storage instead of downloading it again. \n",
    "\n",
    "This is especially useful for larger models or when working in environments with limited internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bb1d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n",
      "Model loaded from local directory !\n"
     ]
    }
   ],
   "source": [
    "# Load model from local directory\n",
    "print(\"Loading model from local directory...\")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "print(\"Model loaded from local directory !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e732f95",
   "metadata": {},
   "source": [
    "## Generating Text with a Local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60cfaa",
   "metadata": {},
   "source": [
    "Create a text generation function that allows to control various parameters :\n",
    "\n",
    "- **Temperature** : Controls randomness (higher = more creative, lower = more deterministic)\n",
    "- **Max length** : The maximum number of tokens to generate\n",
    "- **Top-p (nucleus sampling)** : Limits token selection to a subset of most likely tokens\n",
    "- **Top-k** : Limits selection to the k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d95926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=.8, top_p=.9, top_k=50, do_sample=True):\n",
    "    \"\"\"\n",
    "        Generate text from a prompt with customizable parameters\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input text to continue\n",
    "            max_length (int): Maximum length of generated text (including prompt)\n",
    "            temperature (float): Higher values (>1.0) increase randomness, lower values (<1.0) make it more deterministic\n",
    "            top_p (float): Nucleus sampling parameter (0-1.0)\n",
    "            top_k (int): Limits selection to k most likely tokens\n",
    "            do_sample (bool): If False, uses greedy decoding instead of sampling\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated text including the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    # Prepare the inputs : str to tokens\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = local_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=local_tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean excess whitespace \n",
    "    clean_text = re.sub(r\"\\s+\", \" \", generated_text)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c3eb7",
   "metadata": {},
   "source": [
    "### Experimenting with Different Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32210a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Default parameters (temperature=0.8)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class will be part of LLM Engineering. This course will be part of LLM Engineering. This course will be part of LLM Engineering.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 2: Low temperature (more deterministic)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class is a great opportunity to learn how to build a solid, scalable, scalable, and scalable LLM engineering system. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 3: High temperature (more creative/random)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class combines LLM technology to design the entire LLM Engineering project and is aimed at the technical engineering students who are taking part in the project - the engineering students! Course Overview Graduate Course Overview - Introduction Course Description The course is a 4 x 5 tutorial, focused solely on LLM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 4: Greedy decoding (no sampling, always selects most likely token)\n",
      "Prompt: \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
      "Generated: Welcome to Fundamentals of LLM Engineering course. This class is designed to help you understand the fundamentals of LLM engineering. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Welcome to Fundamentals of LLM Engineering course. This class\"\n",
    "\n",
    "print(\"Example 1: Default parameters (temperature=0.8)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 2: Low temperature (more deterministic)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=0.2, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 3: High temperature (more creative/random)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=1.5, max_length=75)}\")\n",
    "print(\"-\"*100)\n",
    "print()\n",
    "\n",
    "print(\"Example 4: Greedy decoding (no sampling, always selects most likely token)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, top_p=None, temperature=None, do_sample=False, max_length=75)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
